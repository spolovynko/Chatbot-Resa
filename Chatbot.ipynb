{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import spacy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "NER = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "raw_text = 'I would like to book a table for ten people  five at the evening'\n",
    "sent = NER(raw_text)\n",
    "\n",
    "for word in sent.ents:\n",
    "    print(word.text,word.label_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ten CARDINAL\n",
      "five CARDINAL\n",
      "evening TIME\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "for chunk in sent:\n",
    "    print(chunk, chunk.pos_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I PRON\n",
      "would AUX\n",
      "like VERB\n",
      "to PART\n",
      "book VERB\n",
      "a DET\n",
      "table NOUN\n",
      "for ADP\n",
      "ten NUM\n",
      "people NOUN\n",
      "  SPACE\n",
      "five NUM\n",
      "at ADP\n",
      "the DET\n",
      "evening NOUN\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "for chunk in sent.noun_chunks:\n",
    "    print(chunk)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I\n",
      "a table\n",
      "ten people\n",
      "the evening\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import json\n",
    "with open(\"intent.json\",\"r\") as f:\n",
    "    intents = json.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "dico = []\n",
    "corpus = []\n",
    "data = []\n",
    "tags = []\n",
    "\n",
    "def tokenizer(text):\n",
    "    tokens = NER(text)\n",
    "    tokens = [token.lemma_ for token in tokens if token.lemma_.isalpha()]\n",
    "    return tokens\n",
    "\n",
    "for intent in intents['intents']:\n",
    "    tag = intent[('tag')]\n",
    "    dico += tokenizer(tag)\n",
    "    for pattern in intent['patterns']:\n",
    "        tags.append(tag)\n",
    "        pattern_tokens = tokenizer(pattern)\n",
    "        dico += pattern_tokens\n",
    "        data.append([pattern_tokens, tag])\n",
    "        corpus.append(' '.join(pattern_tokens))\n",
    "    for resp in intent['responses']:\n",
    "        dico += tokenizer(resp)\n",
    "        \n"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "dico_tags = {tag : i for (i, tag) in enumerate(set(tags))}\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(corpus)\n",
    "y_train = [dico_tags[tag] for tag in tags]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.n_samples = len(X_train.toarray())\n",
    "        self.x_data = X_train.toarray()\n",
    "        self.y_data = np.array(y_train)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_data[idx], self.y_data[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "batch_size = 8\n",
    "\n",
    "    \n",
    "dataset = ChatDataset()\n",
    "train_loader = DataLoader(dataset=dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=True, \n",
    "                            num_workers=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "hidden_size = 8\n",
    "output_size = len(set(tags))\n",
    "input_size = len(X_train.toarray()[0])\n",
    "learning_rate = 0.001\n",
    "n_epochs=300\n",
    "\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out =self.l1(x.float())\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        return out\n",
    "    \n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.to(device)\n",
    "def binary_accuracy(preds, y):\n",
    "    correct = preds.argmax(axis=1)==y\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        predictions = model.forward(batch[0].to(device))\n",
    "        loss = criterion(predictions, batch[1].to(device))  \n",
    "        acc = binary_accuracy(predictions, batch[1].to(device)) \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for batch in iterator:\n",
    "            predictions = model(batch[0]) \n",
    "            loss = criterion(predictions, batch[1])       \n",
    "            acc = binary_accuracy(predictions, batch[1])\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    #valid_loss, valid_acc = evaluate(model, dataloader_val, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    #if valid_loss < best_valid_loss:\n",
    "    #    best_valid_loss = valid_loss\n",
    "    #    torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    #print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for (words,labels) in train_loader:\n",
    "        words = words.to(device).float()\n",
    "        labels = labels.to(device).float()\n",
    "        \n",
    "        \n",
    "        outputs = model(words).float()\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(f\"epoch: {epoch+1}/{n_epochs}, loss: {loss.item():.4f}\")\n",
    "print(f\"final loss, loss: {loss.item():.4f}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.eval()\n",
    "test_sentence = 'I would like to book a table for tomorow evening'\n",
    "\n",
    "vect_sent = vectorizer.transform([' '.join(tokenizer(test_sentence))]).toarray()\n",
    "output = model(torch.tensor(vect_sent).to(device).float())\n",
    "list(dico_tags.keys())[torch.softmax(output, dim=1).argmax().item()]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "set(tags)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "verbs = ['want', 'would like', 'wish']\n",
    "complements = ['to book a table', 'a table']\n",
    "times = ['this morning', 'for lunch', 'for this evening', 'for tomorow evening']\n",
    "\n",
    "bookings = [f\"I {verb} {complement} {time}\" for verb in verbs for time in times for complement in complements]\n",
    "bookings"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from fuzzywuzzy import process\n",
    "\n",
    "listing = ['Hello', 'Hi', '9', 'like']\n",
    "process.extract('Hlo', listing)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokens = NER(\"I don't want today I want not tomorrow\")\n",
    "\n",
    "print(' '.join(token.lemma_ for token in tokens if not token.is_stop or \"n't\" in token.lemma_))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for token in tokens:\n",
    "    print(token)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python385jvsc74a57bd09977e029b2da6177bb9bc7be3b4cc732c21fce726b7c7753acdf2f8f3db06ed7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}